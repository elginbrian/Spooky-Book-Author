{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef93ed37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menggunakan device: cpu\n",
      "Jumlah data train: 19579\n",
      "Jumlah data test: 8392\n",
      "Contoh data train:\n",
      "        id                                               text author\n",
      "0  id26305  This process, however, afforded me no means of...    EAP\n",
      "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
      "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
      "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
      "4  id12958  Finding nothing else, not even gold, the Super...    HPL\n",
      "Train split: 17621\n",
      "Validation split: 1958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 17621/17621 [00:00<00:00, 20171.75 examples/s]\n",
      "Map: 100%|██████████| 1958/1958 [00:00<00:00, 23611.06 examples/s]\n",
      "Map: 100%|██████████| 8392/8392 [00:00<00:00, 25175.93 examples/s]\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\LENOVO\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 109\u001b[39m\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mlog_loss\u001b[39m\u001b[33m\"\u001b[39m: loss, \u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m: acc}\n\u001b[32m    107\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m# Definisikan TrainingArguments\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m training_args = \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./results\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msteps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_for_best_model\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlog_loss\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgreater_is_better\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\n\u001b[32m    124\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# Buat objek Trainer\u001b[39;00m\n\u001b[32m    128\u001b[39m trainer = Trainer(\n\u001b[32m    129\u001b[39m     model=model,\n\u001b[32m    130\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m     compute_metrics=compute_metrics,\n\u001b[32m    136\u001b[39m )\n",
      "\u001b[31mTypeError\u001b[39m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Notebook Identifikasi Penulis Horor\n",
    "#\n",
    "# Notebook ini menggunakan model BERT untuk melakukan klasifikasi kutipan horor ke dalam tiga kelas:\n",
    "# - **EAP**: Edgar Allan Poe\n",
    "# - **HPL**: H.P. Lovecraft\n",
    "# - **MWS**: Mary Shelley\n",
    "#\n",
    "# Langkah-langkah utama yang dilakukan:\n",
    "# 1. Pemuatan dan eksplorasi data\n",
    "# 2. Pra-pemrosesan dengan tokenizer BERT\n",
    "# 3. Fine-tuning model pre-trained BERT dengan Trainer API\n",
    "# 4. Evaluasi model dengan validasi silang sederhana (train/validation split)\n",
    "# 5. Prediksi pada data test dan pembuatan file submission\n",
    "\n",
    "# %%\n",
    "# Import libraries yang diperlukan\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "\n",
    "# Cek apakah GPU tersedia\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Menggunakan device:\", device)\n",
    "\n",
    "# %%\n",
    "# Pemuatan data\n",
    "# Pastikan file train.csv, test.csv, sample_submission.csv sudah berada di direktori kerja yang sama\n",
    "train_df = pd.read_csv(\"./train/train.csv\")\n",
    "test_df = pd.read_csv(\"./test/test.csv\")\n",
    "sample_submission = pd.read_csv(\"./sample_submission/sample_submission.csv\")\n",
    "\n",
    "print(\"Jumlah data train:\", len(train_df))\n",
    "print(\"Jumlah data test:\", len(test_df))\n",
    "print(\"Contoh data train:\")\n",
    "print(train_df.head())\n",
    "\n",
    "# %%\n",
    "# Ubah label author menjadi angka (misalnya mapping: EAP -> 0, HPL -> 1, MWS -> 2)\n",
    "label2id = {\"EAP\": 0, \"HPL\": 1, \"MWS\": 2}\n",
    "id2label = {0: \"EAP\", 1: \"HPL\", 2: \"MWS\"}\n",
    "train_df[\"label\"] = train_df[\"author\"].map(label2id)\n",
    "\n",
    "# %%\n",
    "# Split data train menjadi train dan validation (misalnya 90:10)\n",
    "train_data, val_data = train_test_split(train_df, test_size=0.1, random_state=42, stratify=train_df[\"label\"])\n",
    "print(\"Train split:\", len(train_data))\n",
    "print(\"Validation split:\", len(val_data))\n",
    "\n",
    "# %%\n",
    "# Konversi ke Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_data.reset_index(drop=True))\n",
    "val_dataset = Dataset.from_pandas(val_data.reset_index(drop=True))\n",
    "test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "# %%\n",
    "# Gunakan model dan tokenizer pre-trained\n",
    "model_name = \"bert-base-uncased\"  # atau bisa gunakan model lain seperti roberta-base\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Fungsi tokenisasi\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=False)  # padding nanti dilakukan secara batch\n",
    "\n",
    "# Tokenisasi dataset\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# %%\n",
    "# Format dataset untuk Trainer (set format tensor dan kolom label)\n",
    "columns_to_remove = [\"author\", \"label\"]  # \"author\" tidak diperlukan lagi pada input\n",
    "train_dataset = train_dataset.remove_columns([\"author\"]).with_format(\"torch\")\n",
    "val_dataset = val_dataset.remove_columns([\"author\"]).with_format(\"torch\")\n",
    "# Untuk test data, tidak ada label\n",
    "test_dataset = test_dataset.with_format(\"torch\")\n",
    "\n",
    "# %%\n",
    "# Buat model dengan 3 label\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=3,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# %%\n",
    "# Data collator untuk padding dinamis\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# %%\n",
    "# Fungsi evaluasi: menghitung log loss dan akurasi\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = torch.softmax(torch.tensor(logits), dim=1).numpy()\n",
    "    loss = log_loss(labels, preds, labels=[0, 1, 2])\n",
    "    pred_labels = np.argmax(preds, axis=1)\n",
    "    acc = accuracy_score(labels, pred_labels)\n",
    "    return {\"log_loss\": loss, \"accuracy\": acc}\n",
    "\n",
    "# %%\n",
    "# Definisikan TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    logging_steps=100,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"log_loss\",\n",
    "    greater_is_better=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Buat objek Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Mulai proses fine-tuning\n",
    "trainer.train()\n",
    "\n",
    "# %%\n",
    "# Evaluasi model pada validation set\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Hasil evaluasi:\", eval_results)\n",
    "\n",
    "# %%\n",
    "# Lakukan prediksi pada data test\n",
    "# Perlu diingat test dataset hanya memiliki kolom \"id\" dan \"text\"\n",
    "# Dapatkan prediksi probabilitas untuk masing-masing kelas\n",
    "predictions = trainer.predict(test_dataset)\n",
    "logits = predictions.predictions\n",
    "# Ubah logits menjadi probabilitas melalui softmax\n",
    "probs = torch.softmax(torch.tensor(logits), dim=1).numpy()\n",
    "\n",
    "# %%\n",
    "# Buat file submission\n",
    "submission_df = pd.DataFrame({\n",
    "    \"id\": test_df[\"id\"],\n",
    "    \"EAP\": probs[:, label2id[\"EAP\"]],\n",
    "    \"HPL\": probs[:, label2id[\"HPL\"]],\n",
    "    \"MWS\": probs[:, label2id[\"MWS\"]]\n",
    "})\n",
    "submission_df.to_csv(\"./sample_submission/submission_v11.csv\", index=False)\n",
    "print(\"File submission telah tersimpan sebagai 'submission.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
